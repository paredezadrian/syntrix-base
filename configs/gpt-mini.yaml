model: gpt_mini
vocab_size: 128
block_size: 128
n_layer: 4
n_head: 4
d_model: 256
mlp_ratio: 4
rope: true
norm: rms

optim:
  lr: 3.0e-3
  weight_decay: 0.1
  betas: [0.9, 0.95]
  grad_clip: 1.0

schedule:
  type: cosine
  warmup_steps: 50

train:
  batch_size: 32
  microbatch: 1
  grad_accum: 64
  dtype: float32
  threads: 4


