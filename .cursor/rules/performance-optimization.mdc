---
description: Performance optimization patterns for deep learning models.
---

# Performance Optimization Patterns

## CPU-First Optimization
- **Thread Management**: Use `torch.set_num_threads()` to control PyTorch threads
- **BLAS Threading**: Pin MKL/OMP threads for reproducible performance
- **Memory Layout**: Optimize tensor memory layout for CPU cache efficiency
- **Batch Processing**: Use appropriate batch sizes for CPU memory hierarchy

## Memory Efficiency
- **Microbatching**: Process very small batches (1-4) to reduce memory usage
- **Gradient Accumulation**: Simulate larger effective batch sizes
- **Gradient Checkpointing**: Trade computation for memory when needed
- **Memory Mapping**: Use mmap for large dataset files

## Training Optimization
- **Mixed Precision**: Support both fp32 and fp64 with appropriate tolerances
- **Gradient Clipping**: Prevent gradient explosion with norm clipping
- **Learning Rate Scheduling**: Use cosine schedule with warmup for stability
- **EMA**: Implement exponential moving average for better final performance

## Profiling & Monitoring
- **Timing**: Use precise timing for training steps and evaluation
- **Throughput**: Track tokens per second for performance monitoring
- **Memory Usage**: Monitor peak memory consumption
- **CPU Utilization**: Track CPU usage and thread efficiency

## Reproducible Performance
- **Fixed Seeds**: Use deterministic random seeds for reproducible results
- **Thread Pinning**: Pin threads to specific CPU cores when possible
- **Environment Logging**: Log all environment variables affecting performance
- **Benchmark Scripts**: Include reproducible benchmark commands

## Compilation & JIT
- **torch.compile**: Use when available for small speedups
- **JIT Scripting**: Apply to small, frequently-called functions
- **Fusion**: Use fused operations when available (e.g., fused attention)
- **Custom Kernels**: Implement custom CUDA kernels only if necessary
