---
description: Model architecture patterns for Apply Intelligently.
---

# Model Architecture Patterns

## Layer Implementation
- All custom layers must inherit from `torch.nn.Module`
- Implement `forward()` method with proper type hints
- Use `torch.nn.Parameter` for learnable weights
- Initialize weights using `torch.nn.init` functions
- Include shape validation in `forward()` method

## Transformer Components (GPT Mini)
- **Pre-LayerNorm**: Apply normalization before attention/MLP blocks
- **SwiGLU**: Use `torch.nn.functional.silu` for activation
- **Rotary Position Embeddings**: Implement in [layers.py](mdc:syntrix/nn/layers.py)
- **Causal Attention**: Mask future tokens in attention computation
- **RMSNorm**: Use root mean square normalization instead of LayerNorm

## SSM Components
- **Selective State Update**: Implement diagonal + convolution shortcut
- **State Dimension**: Keep state size small (typically 16-64)
- **Convolution**: Use 1D convolution for sequence modeling
- **Gating**: Implement selective update mechanism

## RNN Components
- **Gated Architecture**: Use GRU or LSTM-style gating
- **Hidden State**: Maintain hidden state across sequence
- **Output Projection**: Project to vocabulary size

## Model Composition
- Use composition over inheritance for complex models
- Implement `__init__()` with configurable hyperparameters
- Add `forward()` method that handles input shapes
- Include `get_num_params()` method for parameter counting
- Support both training and inference modes

## Memory Efficiency
- Use `torch.jit.script` for small, frequently-called functions
- Implement gradient checkpointing for large models
- Use `torch.utils.checkpoint` for memory-intensive operations
- Support mixed precision training (fp16/fp32)
